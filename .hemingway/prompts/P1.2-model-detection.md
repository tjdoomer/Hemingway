# Specialist Prompt: P1.2 — Model Detection & Auto-Discovery

## Your Role
You are the **Model Detection Specialist**. Your job is to make the model detection system actually discover and classify available models from LMStudio, Ollama, OpenAI, and Anthropic.

## Context
The current `ModelDetector` in `src/models/detector.ts` has working structure but:
- The Anthropic model list is outdated (Claude 3 era, not Claude 4.x)
- No Ollama support exists
- No model tier classification (fast/medium/strong)
- No intelligent routing based on task type

The Delta-code project (our predecessor) has a `ResourceProbe` pattern that auto-discovers Ollama models and classifies them by parameter count. We should adopt this pattern.

## What You Need to Do

### Step 1: Add Ollama detection
```typescript
// In src/models/detector.ts
private async detectOllama(config: ProviderConfig): Promise<ModelInfo[]> {
  const baseUrl = config.baseUrl || 'http://localhost:11434';
  const models: ModelInfo[] = [];

  try {
    const response = await fetch(`${baseUrl}/api/tags`);
    if (!response.ok) return models;

    const data = await response.json() as { models: Array<{ name: string; size: number; details?: { parameter_size?: string } }> };

    for (const model of data.models) {
      const paramSize = this.parseParamSize(model.details?.parameter_size || '');
      const tier = this.classifyTier(paramSize);

      models.push({
        id: `ollama:${model.name}`,
        name: model.name,
        provider: 'ollama',
        isLocal: true,
        capabilities: this.estimateCapabilities(model.name, paramSize),
        isAvailable: true,
      });
    }
  } catch {
    logger.debug('Ollama not available');
  }

  return models;
}

private parseParamSize(sizeStr: string): number {
  // "7B" → 7, "70B" → 70, "1.5B" → 1.5
  const match = sizeStr.match(/([\d.]+)B/i);
  return match ? parseFloat(match[1]) : 0;
}
```

### Step 2: Update Anthropic model list
```typescript
const claudeModels = [
  'claude-opus-4-6',
  'claude-sonnet-4-5-20250929',
  'claude-haiku-4-5-20251001',
];
```

### Step 3: Add model tier classification
```typescript
type ModelTier = 'fast' | 'medium' | 'strong';

private classifyTier(paramBillions: number): ModelTier {
  if (paramBillions <= 8) return 'fast';
  if (paramBillions <= 35) return 'medium';
  return 'strong';
}
```

### Step 4: Implement intelligent routing
```typescript
// Work tasks → cloud (strong tier) preferred, local strong as fallback
getBestWorkModel(): ModelInfo | undefined {
  const cloud = this.getCloudModels();
  const preferred = cloud.find(m =>
    m.name.includes('claude-opus') ||
    m.name.includes('claude-sonnet') ||
    m.name.includes('gpt-4o')
  );
  if (preferred) return preferred;

  // Fallback to strong local model
  const localStrong = this.getLocalModels().filter(m => /* strong tier */);
  return localStrong[0] || cloud[0];
}

// Personal tasks → local (any tier) preferred, cloud haiku/mini as fallback
getBestPersonalModel(): ModelInfo | undefined {
  const local = this.getLocalModels();
  if (local.length > 0) {
    return local.sort((a, b) => b.capabilities.contextWindow - a.capabilities.contextWindow)[0];
  }
  return this.getCloudModels().find(m =>
    m.name.includes('haiku') || m.name.includes('mini') || m.name.includes('flash')
  );
}
```

### Step 5: Add `ollama` to the `ModelProvider` type
In `src/types/index.ts`, verify `'ollama'` is already in the union (it is).

## Files to Modify
- `src/models/detector.ts` — Add Ollama detection, update model list, add tier classification
- `src/types/index.ts` — Verify `ModelProvider` includes `'ollama'`
- `src/models/client.ts` — Add Ollama client configuration (OpenAI-compatible)
- `src/cli.ts` — Add Ollama provider configuration in `initialize()`

## Files to Create
- `src/models/detector.test.ts` — Tests for model detection and tier classification

## Acceptance Criteria
- [ ] `npm run typecheck` passes
- [ ] Ollama models are detected when Ollama is running locally
- [ ] LMStudio models are detected when LMStudio is running
- [ ] Cloud model list is current (Claude 4.x, GPT-4o)
- [ ] Models are classified into tiers
- [ ] Work tasks route to cloud/strong models
- [ ] Personal tasks route to local/fast models
- [ ] Graceful fallback when preferred provider is unavailable

## Branch
Work in: `worktree/p1-model-detection`
